{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from string import punctuation as punct\n",
    "\n",
    "import numpy as np\n",
    "from lxml import html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from gensim.models import Word2Vec, FastText, KeyedVectors\n",
    "\n",
    "from mystem import MyStem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В рамках нормализации для всех данных использовал лемматизацию и токенизацию с помощью mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct += '«»–—'\n",
    "MYSTEM = MyStem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punct(x):\n",
    "    return ' '.join([w.strip(punct) for w in x.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корпус новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = pd.concat(\n",
    "    [pd.read_json(open(os.path.join('data_kw', f_name), encoding='utf-8'), lines=True)\n",
    "     for f_name in os.listdir('data_kw') if f_name.startswith('ru')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7217, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA.content = TRAIN_DATA.content.apply(strip_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "content_lem = MYSTEM.run(list(TRAIN_DATA.content.values), flags='-idln')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA['content_lem'] = content_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TRAIN_DATA', 'rb') as f:\n",
    "    TRAIN_DATA = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные парафраза"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_xml = html.fromstring(open('paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "PARA = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARA.label = PARA.label.map({'0': 1, '1': 2, '-1': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARA['text_1'] = PARA.text_1.apply(strip_punct)\n",
    "PARA['text_2'] = PARA.text_2.apply(strip_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "PARA['text_norm_1'] = MYSTEM.run(list(PARA.text_1.values), flags='-idln')\n",
    "PARA['text_norm_2'] = MYSTEM.run(list(PARA.text_2.values), flags='-idln')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('PARA_DATA', 'rb') as f:\n",
    "    PARA_DATA = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CVECT_NORM = CountVectorizer()\n",
    "TFIDF_NORM = TfidfVectorizer()\n",
    "\n",
    "CVECT = CountVectorizer()\n",
    "TFIDF = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "CVECT_NORM = CVECT_NORM.fit(TRAIN_DATA.content_lem)\n",
    "TFIDF_NORM = TFIDF_NORM.fit(TRAIN_DATA.content_lem)\n",
    "\n",
    "CVECT = CVECT.fit(TRAIN_DATA.content)\n",
    "TFIDF = TFIDF.fit(TRAIN_DATA.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_CVECT_NORM = CVECT_NORM.transform(TRAIN_DATA.content_lem)\n",
    "TRAIN_DATA_TFIDF_NORM = TFIDF_NORM.transform(TRAIN_DATA.content_lem)\n",
    "\n",
    "TRAIN_DATA_CVECT = CVECT.transform(TRAIN_DATA.content)\n",
    "TRAIN_DATA_TFIDF = TFIDF.transform(TRAIN_DATA.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем общий размер вектора 128 для всех моделей.  \n",
    "Для более традиционных размеров 300 (или более) доступных данных, скорее всего, будет недостаточно, что приведет к слишком большому числу обучаемых параметров в соотношении с количеством объектов (для W2V и FastText)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "SVD_CVECT = TruncatedSVD(128).fit(TRAIN_DATA_CVECT_NORM)\n",
    "SVD_TFIDF = TruncatedSVD(128).fit(TRAIN_DATA_TFIDF_NORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NMF_CVECT = NMF(128).fit(TRAIN_DATA_CVECT_NORM)\n",
    "NMF_TFIDF = NMF(128).fit(TRAIN_DATA_TFIDF_NORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NMF_CVECT', 'rb') as f:\n",
    "    NMF_CVECT = pickle.load(f)\n",
    "    \n",
    "with open('NMF_TFIDF', 'rb') as f:\n",
    "    NMF_TFIDF = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "W2V = Word2Vec([x.split() for x in TRAIN_DATA.content_lem.values], iter=30, size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V.wv.save('W2V.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V = KeyedVectors.load('W2V.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('заявление', 0.724164605140686),\n",
       " ('информация', 0.7203260660171509),\n",
       " ('сведение', 0.6698870062828064),\n",
       " ('пресс-релиз', 0.6675968170166016),\n",
       " ('материал', 0.6334848999977112),\n",
       " ('данные', 0.5983684062957764),\n",
       " ('отчет', 0.5942883491516113),\n",
       " ('уведомление', 0.5902847051620483),\n",
       " ('релиз', 0.5874637365341187),\n",
       " ('документ', 0.5692962408065796)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2V.most_similar(positive=['сообщение'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "FT_NORM = FastText(\n",
    "    [x.split() for x in TRAIN_DATA.content_lem.values],\n",
    "    iter=30,\n",
    "    size=128,\n",
    "    min_n=2,\n",
    "    max_n=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_NORM.wv.save('FT_NORM.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_NORM = KeyedVectors.load('FT_NORM.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('авиасообщение', 0.8739902973175049),\n",
       " ('обобщение', 0.8208562135696411),\n",
       " ('заявление', 0.8122040033340454),\n",
       " ('разобщение', 0.8047382235527039),\n",
       " ('уведомление', 0.7661752104759216),\n",
       " ('информация', 0.7655895352363586),\n",
       " ('общение', 0.7631301879882812),\n",
       " ('сообща', 0.759331226348877),\n",
       " ('информирование', 0.7436956167221069),\n",
       " ('сведение', 0.7395753860473633)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FT_NORM.most_similar(positive=['сообщение'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "FT = FastText(\n",
    "    [x.split() for x in TRAIN_DATA.content.values],\n",
    "    iter=30,\n",
    "    size=128,\n",
    "    min_n=2,\n",
    "    max_n=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT.wv.save('FT.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT = KeyedVectors.load('FT.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Сообщение', 0.9153209924697876),\n",
       " ('авиасообщение', 0.9104920029640198),\n",
       " ('заявление', 0.8764517903327942),\n",
       " ('общение', 0.8676607012748718),\n",
       " ('уведомление', 0.8466607928276062),\n",
       " ('оповещение', 0.8172410726547241),\n",
       " ('видеообращение', 0.8165761232376099),\n",
       " ('освещение', 0.8085266351699829),\n",
       " ('кровообращение', 0.8066942691802979),\n",
       " ('очищение', 0.805307149887085)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FT.most_similar(positive=['сообщение'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функционал для сборки датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizer_map(vectorizer):\n",
    "    return {w: i for i, w in enumerate(vectorizer.get_feature_names())}\n",
    "\n",
    "\n",
    "def get_tfidf_weights(text, tfidf, tfidf_map):\n",
    "    vector = tfidf.transform([text]).toarray()[0]\n",
    "    \n",
    "    return {w: vector[tfidf_map[w]] if w in tfidf_map else 1.0 for w in text.split()}\n",
    "    \n",
    "\n",
    "def get_w2v_embedding(text, model, tfidf=None, tfidf_map=None):\n",
    "    vectors = list()\n",
    "    weights = get_tfidf_weights(text, tfidf, tfidf_map) if tfidf is not None else {w: 1.0 for w in text.split()}\n",
    "    \n",
    "    for w in text.split():\n",
    "        if w in model:\n",
    "            vectors.append(model[w] * weights[w])\n",
    "    \n",
    "    return np.mean(vectors, axis=0) if vectors else np.random.normal(size=(model.vector_size,))\n",
    "\n",
    "\n",
    "def compute_decomposition_features(para, models, vectorizers):\n",
    "    X = np.zeros((para.shape[0], len(models)))\n",
    "    i = 0\n",
    "    \n",
    "    for model, vectorizer in zip(models, vectorizers):\n",
    "        V1 = model.transform(vectorizer.transform(para.text_norm_1))\n",
    "        V2 = model.transform(vectorizer.transform(para.text_norm_2))\n",
    "        \n",
    "        X[:,i] = np.diag(cosine_similarity(V1, V2))\n",
    "        i += 1\n",
    "        \n",
    "    return X\n",
    "\n",
    "\n",
    "def compute_w2v_features(para, model, tfidf, tfidf_map):\n",
    "    V1 = np.array([get_w2v_embedding(x, model) for x in para.text_norm_1])\n",
    "    V2 = np.array([get_w2v_embedding(x, model) for x in para.text_norm_2])\n",
    "    \n",
    "    VW1 = np.array([get_w2v_embedding(x, model, tfidf, tfidf_map) for x in para.text_norm_1])\n",
    "    VW2 = np.array([get_w2v_embedding(x, model, tfidf, tfidf_map) for x in para.text_norm_2])\n",
    "    \n",
    "    return np.hstack([np.diag(cosine_similarity(V1, V2)).reshape(-1, 1),\n",
    "                      np.diag(cosine_similarity(VW1, VW2)).reshape(-1, 1)\n",
    "                     ]\n",
    "    )\n",
    "    \n",
    "    \n",
    "def compute_ft_features(para, model, model_norm, tfidf, tfidf_map, tfidf_norm, tfidf_norm_map):\n",
    "    V1 = np.array([get_w2v_embedding(x, model) for x in para.text_1])\n",
    "    V2 = np.array([get_w2v_embedding(x, model) for x in para.text_2])\n",
    "    \n",
    "    VW1 = np.array([get_w2v_embedding(x, model, tfidf, tfidf_map) for x in para.text_1])\n",
    "    VW2 = np.array([get_w2v_embedding(x, model, tfidf, tfidf_map) for x in para.text_2])\n",
    "    \n",
    "    V1_NORM = np.array([get_w2v_embedding(x, model_norm) for x in para.text_norm_1])\n",
    "    V2_NORM = np.array([get_w2v_embedding(x, model_norm) for x in para.text_norm_2])\n",
    "    \n",
    "    VW1_NORM = np.array([get_w2v_embedding(x, model_norm, tfidf_norm, tfidf_norm_map) for x in para.text_norm_1])\n",
    "    VW2_NORM = np.array([get_w2v_embedding(x, model_norm, tfidf_norm, tfidf_norm_map) for x in para.text_norm_2])\n",
    "    \n",
    "    return np.hstack([np.diag(cosine_similarity(V1, V2)).reshape(-1, 1),\n",
    "                      np.diag(cosine_similarity(VW1, VW2)).reshape(-1, 1),\n",
    "                      np.diag(cosine_similarity(V1_NORM, V2_NORM)).reshape(-1, 1),\n",
    "                      np.diag(cosine_similarity(VW1_NORM, VW2_NORM)).reshape(-1, 1)\n",
    "                     ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_MAP = get_vectorizer_map(TFIDF)\n",
    "TFIDF_NORM_MAP = get_vectorizer_map(TFIDF_NORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_decomp = compute_decomposition_features(\n",
    "    para=PARA_DATA,\n",
    "    models=[SVD_CVECT,SVD_TFIDF, NMF_CVECT, NMF_TFIDF],\n",
    "    vectorizers=[CVECT_NORM, TFIDF_NORM, CVECT_NORM, TFIDF_NORM]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 4)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_decomp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_w2v = compute_w2v_features(PARA_DATA, W2V, TFIDF_NORM, TFIDF_NORM_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 2)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_ft = compute_ft_features(\n",
    "    PARA_DATA, FT, FT_NORM, TFIDF, TFIDF_MAP, TFIDF_NORM, TFIDF_NORM_MAP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 4)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([X_decomp, X_w2v, X_ft])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 10)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на logreg и RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=5, scoring='f1_micro', class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5058633943894317"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(lr, X, PARA_DATA.label, scoring='f1_micro', cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000, max_depth=100, \n",
    "                            max_features=None, criterion='entropy',\n",
    "                            n_jobs=3, class_weight='balanced'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5506822575383388"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(rf, X, PARA_DATA.label, scoring='f1_micro', cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, качество не самое высокое, однако лучше случайного ответа.  \n",
    "Вероятно, перебор гиперпараметров поможет несколько улучшить положение.  \n",
    "Далее я произведу подбор гиперпараметров по минимальной схеме: 5 параметров по паре значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(models, est):\n",
    "    X_decomp = compute_decomposition_features(\n",
    "        para=PARA_DATA,\n",
    "        models=[models['SVD_CVECT'], models['SVD_TFIDF'], models['NMF_CVECT'], models['NMF_TFIDF']],\n",
    "        vectorizers=[CVECT_NORM, TFIDF_NORM, CVECT_NORM, TFIDF_NORM]\n",
    "    )\n",
    "\n",
    "    X_w2v = compute_w2v_features(PARA_DATA, models['W2V'], TFIDF_NORM, TFIDF_NORM_MAP)\n",
    "\n",
    "    X_ft = compute_ft_features(\n",
    "        PARA_DATA, models['FT'], models['FT_NORM'], TFIDF, TFIDF_MAP, TFIDF_NORM, TFIDF_NORM_MAP\n",
    "    )\n",
    "    \n",
    "    X = np.hstack([X_decomp, X_w2v, X_ft])\n",
    "    \n",
    "    return cross_val_score(est, X, PARA_DATA.label, scoring='f1_micro', cv=5).mean()\n",
    "\n",
    "\n",
    "MODELS = {\n",
    "    'SVD_CVECT': SVD_CVECT,\n",
    "    'SVD_TFIDF': SVD_TFIDF,\n",
    "    'NMF_CVECT': NMF_CVECT,\n",
    "    'NMF_TFIDF': NMF_TFIDF,\n",
    "    'W2V': W2V,\n",
    "    'FT_NORM': FT_NORM,\n",
    "    'FT': FT\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличим кол-во компонент для `SVD`, рассмотрев значения 128 (уже построено выше) и 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD_CVECT2 = TruncatedSVD(256).fit(CVECT_NORM.transform(TRAIN_DATA.content_lem))\n",
    "SVD_TFIDF2 = TruncatedSVD(256).fit(TFIDF_NORM.transform(TRAIN_DATA.content_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS.update({'SVD_CVECT': SVD_CVECT2, 'SVD_TFIDF': SVD_TFIDF2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5036484773200627"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "run(MODELS, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS.update({'SVD_CVECT': SVD_CVECT, 'SVD_TFIDF': SVD_TFIDF})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С 256 компонентами прирост получить не удалось."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для `W2V` увеличим размер векторов до 256 (размера 128 уже построены), а затем рассмотрим оба алгоритма: `cbow` и `skipgram`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "W2V2 = Word2Vec([x.split() for x in TRAIN_DATA.content_lem.values], iter=30, size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS.update({'W2V': W2V2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5080754403119105"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "run(MODELS, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С размером векторов в 256 есть прирост + 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "W2V3 = Word2Vec([x.split() for x in TRAIN_DATA.content_lem.values], iter=20, sg=1, size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS.update({'W2V': W2V3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49977342700427985"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "run(MODELS, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS.update({'W2V': W2V2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако смена алгоритма с `cbow` на `skipgram` ухудшила качество, что соотносится с рекомендациями Миколова и Ко использовать `cbow` на не достаточно больших данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, рассмотрим гипепараметры `min_n`, а затем `max_n` для FastText на ненормализованных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "FT2 = FastText(\n",
    "    [x.split() for x in TRAIN_DATA.content.values],\n",
    "    iter=20,\n",
    "    size=128,\n",
    "    min_n=2,\n",
    "    max_n=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS.update({'FT': FT2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5075225723091462"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "run(MODELS, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "FT3 = FastText(\n",
    "    [x.split() for x in TRAIN_DATA.content.values],\n",
    "    iter=20,\n",
    "    size=128,\n",
    "    min_n=3,\n",
    "    max_n=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS.update({'FT': FT3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5030927413499728"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "run(MODELS, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "FT4 = FastText(\n",
    "    [x.split() for x in TRAIN_DATA.content.values],\n",
    "    iter=20,\n",
    "    size=256,\n",
    "    min_n=2,\n",
    "    max_n=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS.update({'FT': FT4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 56.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.509874750257616"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "run(MODELS, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование векторов размера 256 и n-грамм от 2 до 4 позволило получить еще небольшой прирост"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого, по итогам такого условного перебора удалось добиться прироста качества +~0.005.  \n",
    "\n",
    "Рассмотренные гиперпараметры:  \n",
    "1. SVD.n_components: __128__, 256\n",
    "2. W2V.size: 128, __256__\n",
    "3. W2V.sg: __0__, 1\n",
    "4. FastText.min_n: __2__, 3\n",
    "4. FastText.max_n: __4__, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

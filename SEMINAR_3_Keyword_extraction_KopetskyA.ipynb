{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# скачаем данные в папке data и распакуем их\n",
    "# Данные -- ng\n",
    "PATH_TO_DATA = './data_kw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(PATH_TO_DATA, file) for file in os.listdir(PATH_TO_DATA)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим файлы в один датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([pd.read_json(file, lines=True, encoding='utf-8') for file in files], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1987, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vectorizer=None,\n",
    "                 lemmatize=True,\n",
    "                 preserve_case=True,\n",
    "                 parse_named_entities=True,\n",
    "                 w2v_model=None,\n",
    "                 allowed_pos_tags=['NOUN']):\n",
    "        \n",
    "        self.vectorizer = vectorizer\n",
    "        self.lemmatize = lemmatize\n",
    "        self.preserve_case = preserve_case\n",
    "        self.parse_named_entities = parse_named_entities\n",
    "        self.w2v_model = w2v_model\n",
    "        self.allowed_pos_tags = allowed_pos_tags\n",
    "        \n",
    "        # named entities\n",
    "        self.NE = defaultdict(set)\n",
    "        self.W_2_VECT_IDX= dict()\n",
    "        self.PREDICT_BY_MAP = {'most_common_ne': self.__predict_most_common_ne,\n",
    "                               'tfidf': self.__predict_tfidf,\n",
    "                               'tfidf_ne': self.__predict_tfidf_ne,\n",
    "                               'centrality': self.__predict_centrality\n",
    "        }\n",
    "        self.morph = MorphAnalyzer()\n",
    "        \n",
    "    def normalize(self, text, _id=None):\n",
    "        normalized = list()\n",
    "        capitalization = list()\n",
    "        \n",
    "        for sent in sent_tokenize(text):\n",
    "            for i, word in enumerate(sent.strip().split()):\n",
    "                word = re.sub('^[\\W]*', '', word)\n",
    "                word = re.sub('[\\W]*$', '', word)\n",
    "\n",
    "                if not word or word.lower() in STOPWORDS:\n",
    "                    continue\n",
    "                \n",
    "                normalized.append(word.lower())\n",
    "                capitalization.append(True if word[0].isupper() and i > 0 else False)\n",
    "        \n",
    "        if self.lemmatize:\n",
    "            if self.allowed_pos_tags is not None:\n",
    "                _normalized = list()\n",
    "                _capitalization = list()\n",
    "                \n",
    "                normalized = [self.morph.parse(word)[0] for word in normalized]\n",
    "                \n",
    "                for i, word in enumerate(normalized):\n",
    "                    if word.tag.POS in self.allowed_pos_tags:\n",
    "                        _normalized.append(word)\n",
    "                        _capitalization.append(capitalization[i])\n",
    "            \n",
    "                normalized = [word.normal_form for word in _normalized]\n",
    "                capitalization = _capitalization\n",
    "            \n",
    "            else:\n",
    "                normalized = [self.morph.parse(word)[0].normal_form for word in normalized]\n",
    "            \n",
    "        if self.parse_named_entities and _id is not None:\n",
    "            self.NE[_id] = set([word for i, word in enumerate(normalized) if capitalization[i]])\n",
    "        \n",
    "        return ' '.join(normalized)\n",
    "\n",
    "    def scan_ne(self, text):\n",
    "        text = text.strip()\n",
    "        nes = list() \n",
    "\n",
    "        for word in text.split()[1:]:\n",
    "            if word[0].isupper():\n",
    "                nes.append(word)\n",
    "\n",
    "        return nes\n",
    "\n",
    "\n",
    "    def parse_ne(self, docs):\n",
    "        _map = defaultdict(list)\n",
    "\n",
    "        for i, doc in enumerate(docs):\n",
    "            for sent in doc.split():\n",
    "                _map[i].extend(self.scan_ne(sent))\n",
    "\n",
    "        return _map\n",
    "    \n",
    "    def build(self, documents):\n",
    "        self.normalized = [self.normalize(doc, i) for i, doc in enumerate(documents)]\n",
    "        \n",
    "    def load(self, normalized_path='NORMALIZED', named_entities_path='NE'):\n",
    "        with open(normalized_path, 'rb') as f:\n",
    "            self.normalized = pickle.load(f)\n",
    "        \n",
    "        if named_entities_path is not None:\n",
    "            with open(named_entities_path, 'rb') as f:\n",
    "                self.NE = pickle.load(f)\n",
    "                \n",
    "    def fit_vectorizer(self, build_feature_idxs=False):\n",
    "        self.vectorizer.fit(self.normalized)\n",
    "        self.features = self.vectorizer.get_feature_names()\n",
    "        \n",
    "        #if build_feature_idxs:\n",
    "        #    for k, v in self.NE.items():\n",
    "        #        for word in v:\n",
    "        #            if self.W_2_VECT_IDX.get(word) is None:\n",
    "        #                try:\n",
    "        #                    self.W_2_VECT_IDX[word] = self.features.index(word)\n",
    "        #                \n",
    "        #                except ValueError:\n",
    "        #                    continue\n",
    "        \n",
    "        if build_feature_idxs:\n",
    "            for doc in self.normalized:\n",
    "                for word in doc.split():\n",
    "                    if self.W_2_VECT_IDX.get(word) is None:\n",
    "                        try:\n",
    "                            self.W_2_VECT_IDX[word] = self.features.index(word)\n",
    "                        \n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                            \n",
    "        self.features = np.array(self.features)\n",
    "        \n",
    "    def __predict_most_common_ne(self, text, _id, n=10):\n",
    "        nes = [word for word in text.split() if word in self.NE[_id]]\n",
    "        return [word for word, freq in Counter(nes).most_common(n)]\n",
    "    \n",
    "    def __predict_tfidf(self, text, _id, n=10):\n",
    "        return list(self.features[np.argsort(self.vectorizer.transform([text]).toarray()[0])][-n:])\n",
    "    \n",
    "    def __filter_by_cosine(self, predicted, cos_thresh):\n",
    "        \"\"\"return words to filter out\"\"\"\n",
    "        \n",
    "        filtered = set()\n",
    "        \n",
    "        for w1, w2 in combinations(predicted, 2):\n",
    "            pre_filtered = False\n",
    "            \n",
    "            try:\n",
    "                self.w2v_model.wv[w1]\n",
    "            \n",
    "            except KeyError:\n",
    "                filtered.add(w1)\n",
    "                pre_filtered = True\n",
    "            \n",
    "            try:\n",
    "                self.w2v_model.wv[w2]\n",
    "            \n",
    "            except KeyError:\n",
    "                filtered.add(w2)\n",
    "                pre_filtered = True\n",
    "\n",
    "            if not pre_filtered and self.w2v_model.wv.similarity(w1, w2) > cos_thresh:\n",
    "                filtered.update({w1, w2})\n",
    "\n",
    "        return predicted.difference(filtered)\n",
    "\n",
    "    \n",
    "    def __predict_tfidf_ne(self, text, _id, n=10, coef=1, cosine_threshold=None):\n",
    "        transformed = self.vectorizer.transform([text]).toarray()[0]\n",
    "        \n",
    "        for ne in self.NE[_id]:\n",
    "            if self.W_2_VECT_IDX.get(ne) is not None:\n",
    "                transformed[self.W_2_VECT_IDX[ne]] *= coef\n",
    "        \n",
    "        predicted = set(self.features[np.argsort(transformed)][-n:])\n",
    "        \n",
    "        if cosine_threshold is not None:\n",
    "            predicted = self.__filter_by_cosine(predicted, cosine_threshold)\n",
    "            \n",
    "        return predicted\n",
    "    \n",
    "    def __build_graph(self, text, _id, window_size=5, coef=1):\n",
    "        text = text.split()\n",
    "        vocab = set(text)\n",
    "        word2id = {w:i for i, w in enumerate(vocab)}\n",
    "        id2word = {i:w for i, w in enumerate(vocab)}\n",
    "        # преобразуем слова в индексы для удобства\n",
    "        ids = [word2id[word] for word in text]\n",
    "        \n",
    "        transformed = self.vectorizer.transform([' '.join(vocab)]).toarray()[0]\n",
    "        \n",
    "        id2coef = dict()\n",
    "        for i, word in id2word.items():\n",
    "            if self.W_2_VECT_IDX.get(word) is not None:\n",
    "                if word in self.NE[_id]:\n",
    "                    id2coef[i] = transformed[self.W_2_VECT_IDX.get(word)] * coef\n",
    "                \n",
    "                else:\n",
    "                    id2coef[i] = transformed[self.W_2_VECT_IDX.get(word)]\n",
    "            \n",
    "            else:\n",
    "                id2coef[i] = 1\n",
    "\n",
    "        # создадим матрицу совстречаемости\n",
    "        m = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "        \n",
    "        #for i in range(0, len(ids), window_size):\n",
    "        i = 0\n",
    "        z = window_size\n",
    "        while i < len(ids):\n",
    "            window = ids[i:z]\n",
    "            \n",
    "            for j, k in combinations(window, 2):\n",
    "                coef_j = id2coef[j] if id2coef[j] is not None else 1 \n",
    "                coef_k = id2coef[k] if id2coef[k] is not None else 1\n",
    "                value = coef_j + coef_k\n",
    "                \n",
    "                m[j][k] += value\n",
    "                m[k][j] += value\n",
    "            \n",
    "            i += 1\n",
    "            z += 1\n",
    "        \n",
    "        return m, id2word\n",
    "    \n",
    "    def __predict_centrality(self, text, _id, window_size=5, coef=1, n=5):\n",
    "        graph, id2word = self.__build_graph(text, _id, window_size, coef)\n",
    "        degrees = dict()\n",
    "        \n",
    "        for i in range(graph.shape[0]):\n",
    "            degrees[i] = graph[i].sum()\n",
    "\n",
    "        return [id2word[k] for k in sorted(degrees, key=degrees.get)[-n:]]\n",
    "        \n",
    "    def predict(self, idxs=None, by='most_common_ne', predictor_kwargs={'n': 10}):\n",
    "        predictor = self.PREDICT_BY_MAP[by]\n",
    "        \n",
    "        if idxs is not None:\n",
    "            return [predictor(self.normalized[idx], idx, **predictor_kwargs) for idx in idxs]\n",
    "        \n",
    "        else:\n",
    "            return [predictor(doc, i, **predictor_kwargs) for i, doc in enumerate(self.normalized)]\n",
    "                \n",
    "\n",
    "def evaluate(true_kws, predicted_kws):\n",
    "    assert len(true_kws) == len(predicted_kws)\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    jaccards = []\n",
    "    \n",
    "    for i in range(len(true_kws)):\n",
    "        true_kw = set(true_kws[i])\n",
    "        predicted_kw = set(predicted_kws[i])\n",
    "        \n",
    "        tp = len(true_kw & predicted_kw)\n",
    "        union = len(true_kw | predicted_kw)\n",
    "        fp = len(predicted_kw - true_kw)\n",
    "        fn = len(true_kw - predicted_kw)\n",
    "        \n",
    "        if (tp+fp) == 0:\n",
    "            prec = 0\n",
    "        else:\n",
    "            prec = tp / (tp + fp)\n",
    "        \n",
    "        if (tp+fn) == 0:\n",
    "            rec = 0\n",
    "        else:\n",
    "            rec = tp / (tp + fn)\n",
    "        if (prec+rec) == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = (2*(prec*rec))/(prec+rec)\n",
    "            \n",
    "        jac = tp / union\n",
    "        \n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "        jaccards.append(jac)\n",
    "    print('Precision - ', round(np.mean(precisions), 2))\n",
    "    print('Recall - ', round(np.mean(recalls), 2))\n",
    "    print('F1 - ', round(np.mean(f1s), 2))\n",
    "    print('Jaccard - ', round(np.mean(jaccards), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем объект, реализующий корпус и весь необходимый функционал"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(r'E:\\w2v\\araneum\\araneum_none_fasttextskipgram_300_5_2018.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(vectorizer=TfidfVectorizer(), w2v_model=w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus.build(data.content.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pickled\n",
    "# corpus.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus.fit_vectorizer(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В основе моих идей по улучшению качества лежит использование информации об именованных сущностях, потому как в этом датасете таковые составляют большую долю среди всех целевых ключевых слов, а следовательно, при построении модели имеет смысл отдавать им приоритет  \n",
    "Упрощая, под именованой сущностью я понимал любое не первое в предложении вхождение с заглавной буквы (в целом, несмотря на незамысловатость метода, получилось довольно правдоподобно)  \n",
    "Перед экспериментами дынные были нормализованы и лемматизированы, а также были удалены стоп-слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эксперимент 1  \n",
    "F1: 0.17 (прирост относительно baseline: +0.01)    \n",
    "\n",
    "Модификация графового метода: построение графа совместной встречаемости слов в пределе некоторого окна, однако вместо единичек в соответствующую клетку графового представления отправляется сумма tf-idf весов.  \n",
    "Помимо этого, именованные сущности дополнительно взвешиваются некоторой константной, что тем самым позволяет отдавать им некоторый приоритет (параметр `coef`)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.18\n",
      "Recall -  0.18\n",
      "F1 -  0.17\n",
      "Jaccard -  0.1\n"
     ]
    }
   ],
   "source": [
    "evaluate(data.keywords.values, corpus.predict(by='centrality', predictor_kwargs={'coef': 2.5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эксперимент 2  \n",
    "F1: 0.18 (прирост относительно baseline: +0.02)\n",
    "    \n",
    "Модификации метода, на основе ранжирования по tf-idf весам: в качестве ключевых слов возьмем топ-n слов с наибольшим tf-idf весом, при этом дополнительно взвесим именованные сущности некоторой константой (параметр `coef`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.14\n",
      "Recall -  0.28\n",
      "F1 -  0.18\n",
      "Jaccard -  0.11\n"
     ]
    }
   ],
   "source": [
    "evaluate(data.keywords.values, corpus.predict(by='tfidf_ne', predictor_kwargs={'coef': 1.5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эксперимент 3  \n",
    "F1: 0.2 (прирост относительно baseline: +0.04)\n",
    "    \n",
    "Вариация эксперимента 3 с отличием в том, что в качестве предсказанных ключевых слов берутся топ-5  \n",
    "\n",
    "*(ясно, что вариация размера топа не кажется надежным методом, однако в целом оценка качества, предложенная в этом задании, носит в некоторой степени спорный характер, потому как для каждого объекта в датасете число ключевых слов разнится (где-то 3, где-то 20))*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.21\n",
      "Recall -  0.21\n",
      "F1 -  0.2\n",
      "Jaccard -  0.12\n"
     ]
    }
   ],
   "source": [
    "evaluate(data.keywords.values, corpus.predict(by='tfidf_ne', predictor_kwargs={'n': 5, 'coef': 1.5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эксперимент 4 (неудачный)\n",
    "\n",
    "Я также попробовал реализовать предложенную идею фильтрации близких по смыслу слов среди ключевых.  \n",
    "\n",
    "Для этого я ввел некоторый порог косинусной близости между векторами слов, при превышении которого слова считались близкими по смыслу (были взяты уже построенные векторы с rusvectores).  \n",
    "Так, в рамках каждого из рассмотренных выше методов к полученному топу применялась описываемая фильтрация, что однако не позволило получить прирост качества (функционал реализован в `Corpus.__filter_by_cosine`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('NORMALIZED', 'wb') as f:\n",
    "    pickle.dump(corpus.normalized, f)\n",
    "    \n",
    "with open('NE', 'wb') as f:\n",
    "    pickle.dump(corpus.NE, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В семинаре установлен такой бейзлан - F1 -  0.16 (не будем учитывать точность и полноту по отдельности и отбросим жаккара)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ваша задача - предложить 3 способа побить бейзлайн. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нет никаких ограничений кроме:\n",
    "\n",
    "1) нельзя изменять метрику\n",
    "2) решение должно быть воспроизводимым"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве ответа нужно предоставить jupyter тетрадку с экспериментами (обязательное условие!) и описать каждую из идей в форме - https://goo.gl/forms/Zb89yjXFr37EITMq1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый реализованный и описанный способ оценивается в 3 балла. Дополнительный балл можно получить, если способы затрагивают разные аспекты решения (например, первая идея - улучшить нормализацию, вторая - улучшить способ представления текста в виде графа, третья - предложить способ удаления из топа идентичных ключевых слов (рф, россия))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно использовать мой код как основу, а можно придумать что-то полностью другой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если у вас никак не получается побить бейзлайн вы можете предоставить реализацию и описание неудавшихся экспериментов (каждый оценивается в 0.5 баллов)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
